# -*- coding: utf-8 -*-
"""Major_US_Companies_Stocks_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RdoC49vpsKJ8nAzKqmD8A4eBr0ghg7eZ
"""

# Commented out IPython magic to ensure Python compatibility.
#Importing the Libraries
import pandas as pd
import numpy as np
# %matplotlib inline
import matplotlib. pyplot as plt
import matplotlib
from sklearn.preprocessing import MinMaxScaler
from keras.layers import LSTM, Dense, Dropout
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib. dates as mandates
from sklearn.preprocessing import MinMaxScaler
from sklearn import linear_model
from keras.models import Sequential
from keras.layers import Dense
import keras.backend as K
#from keras.optimizers import adam_v2
from keras.callbacks import EarlyStopping
from keras.models import load_model
from keras.layers import LSTM
from keras.utils.vis_utils import plot_model

#Get the Dataset
df=pd.read_csv("USstocks19_to_23.csv")
#Check the head of the dataset
df.head()

#Check dataset information e.g number of rows,columns, size etc.
df.info()

#Check some basic statistics of the dataset
df.describe()

#Check the last 3 rows of the dataset
df.tail(3)

#visualizer for Adj Close
df["Adj Close"].plot()

#visualizer for Close
df["Close"].plot()

#coverting buy and sell to 0s and 1s for the miodel
mapping = {"buy": 0, "sell": 1}

#mapping the function
df["Classify"]= df["Classify"].map(mapping)

df["Classify"].value_counts()

# Set the date column as the index.
df = df.set_index('Date')

# Convert the index to pandas DateTimeIndex
df.index = pd.to_datetime(df.index)

# Split the data into features (X) and target variable (y)
X = df[['Close']]
y = df[['Classify']]

# Determine the splitting point for the test set (last 6 months)
split_date = df.index[-1] - pd.DateOffset(months=6)

# Split the data into train and test sets
X_train = X[df.index < split_date]
y_train = y[df.index < split_date]
X_test = X[df.index >= split_date]
y_test = y[df.index >= split_date]

# Print the train and test set sizes
print("Train set size:", len(X_train))
print("Test set size:", len(X_test))

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# Scale the data.
scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X_train)
X_test_normalized = scaler.fit_transform(X_test)


# Reshape the input data for LSTM model
X_train = X_normalized.reshape((X_normalized.shape[0], X_normalized.shape[1], 1))
X_test = X_test_normalized.reshape((X_test_normalized.shape[0], X_test_normalized.shape[1], 1))

# Print the shapes of the train and test sets.
print(X_train.shape)
print(X_test.shape)

import keras
from keras.models import Sequential
from keras.layers import LSTM, Dense

# Create the model.
model = Sequential()
model.add(LSTM(128, input_shape=(1, X_train.shape[2])))
model.add(Dense(1, activation='sigmoid'))

# Compile the model.
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model.
model.fit(X_train, y_train, epochs=10)

# Evaluate the model.
loss = model.evaluate(X_test, y_test)
print('Test loss:', loss)

# Make predictions.
predictions = model.predict(X_test)

from sklearn.metrics import mean_squared_error

# Calculate the mean squared prediction error
mspe = mean_squared_error(y_test, predictions)

# Print the MSPE
print("MSPE:", mspe)

#Saving model to a file
import pickle

with open('model.pkl', 'wb') as file:
    pickle.dump(model, file)

#Categorizing
threshold = 0.5
buy_sell_predictions = np.where(predictions > threshold, 'Buy', 'Sell')

# Determine the splitting point for the test set (last 6 months)
split_date = df.index[-1] - pd.DateOffset(months=6)

# Split the data into train and test sets
X_train = X[df.index < split_date]
y_train = y[df.index < split_date]
X_test = X[df.index >= split_date]
y_test = y[df.index >= split_date]

#predictions categorized into the top 5 buy and sell
symbols_test = df.loc[X_test.index, 'Symbol']
catego = pd.DataFrame()

catego["Symbol"] = symbols_test

catego["Actual values"] = y_test

catego['Predictions'] = predictions

top_5_buys = catego.groupby('Symbol').apply(lambda x: x.nlargest(5, 'Predictions'))
top_5_sells = catego.groupby('Symbol').apply(lambda x: x.nsmallest(5, 'Predictions'))

catego.to_csv("catego.csv")

accuracy_rate = (predictions == y_test).mean()
print('Accuracy rate:', accuracy_rate)



# Determine the splitting point for the test set (last 6 months)
split_date = df.index[-1] - pd.DateOffset(months=6)

# Split the data into train and test sets
X_train = X[df.index < split_date]
y_train = y[df.index < split_date]
X_test = X[df.index >= split_date]
y_test = y[df.index >= split_date]

# Print the train and test set sizes
print("Train set size:", len(X_train))
print("Test set size:", len(X_test))

from statsmodels.tsa.statespace.sarimax import SARIMAX

train_data = pd.concat([X_train, y_train], axis=1)
test_data = pd.concat([X_test, y_test], axis=1)

# Fit the SARIMA model to the training data
model = SARIMAX(train_data['Classify'], order=(1, 0, 1), seasonal_order=(0, 0, 0, 0))
model_fit = model.fit()

# Predict using the trained model on the test data
sarima_pred = model_fit.predict(start=len(train_data), end=len(train_data) + len(test_data) - 1)

sarima_mspe = mean_squared_error(y_test, sarima_pred)

print(sarima_mspe)

final_results= pd.DataFrame()
models =["LSTM", "SARIMAX"]
joint_mspe = [mspe, sarima_mspe]
final_results["Model"] = models
final_results["MSPE"] = joint_mspe
final_results

pip show tensorflow

